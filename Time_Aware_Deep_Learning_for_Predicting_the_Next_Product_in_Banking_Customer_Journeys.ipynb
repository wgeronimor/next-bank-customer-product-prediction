{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timedelta\n",
        "\n",
        "print(f\"Installing libraries... {datetime.now()}\")\n",
        "!pip install openpyxl --quiet\n",
        "!pip install tensorflow scikit-learn --quiet\n",
        "!pip install imbalanced-learn --quiet\n",
        "!pip install xgboost --quiet\n",
        "\n",
        "print(f\"Loading require libraries... {datetime.now()}\")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout, BatchNormalization, Input, Flatten, GlobalAveragePooling1D\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "print(f\"Libraries Loaded!... {datetime.now()}\")"
      ],
      "metadata": {
        "id": "ER7iyT8C1JfQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf99498f-63b6-4c3d-a4ee-43212380d193"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing libraries... 2025-04-01 05:09:18.697089\n",
            "Loading require libraries... 2025-04-01 05:09:24.421373\n",
            "Libraries Loaded!... 2025-04-01 05:09:28.256539\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzQyrllkhF19",
        "outputId": "3194ecd2-44da-4fbb-cc76-a68909eb297c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading data... 2025-04-01 05:09:28.276780\n",
            "data shape: (370793, 10)\n",
            "data loaded... 2025-04-01 05:10:23.303597\n",
            "print_max_min started... 2025-04-01 05:10:23.303775\n",
            "Original Dataset!... 2025-04-01 05:10:23.303786\n",
            "\n",
            "Column Min & Max values after correction:\n",
            "income_group: min=0, max=1111111110000\n",
            "age_group: min=20, max=130\n",
            "cantidad_transacciones: min=1.0, max=16621.0\n",
            "monto_transaccion: min=0.01, max=2325270500.0\n",
            "fecha_apertura: min_year=1987, max_year=2025\n",
            "print_max_min finished... 2025-04-01 05:10:23.316893\n",
            "removing outliers started... 2025-04-01 05:10:23.328484\n",
            "removing outliers finished... 2025-04-01 05:10:24.400048\n",
            "print_max_min started... 2025-04-01 05:10:24.400390\n",
            "Dataset with Outliers Removed!... 2025-04-01 05:10:24.400407\n",
            "\n",
            "Column Min & Max values after correction:\n",
            "income_group: min=0, max=800000\n",
            "age_group: min=20, max=70\n",
            "cantidad_transacciones: min=1.0, max=528.7199999999721\n",
            "monto_transaccion: min=0.01, max=8275667.881199925\n",
            "fecha_apertura: min_year=2011, max_year=2025\n",
            "print_max_min finished... 2025-04-01 05:10:24.424398\n",
            "preparing data... 2025-04-01 05:10:24.424554\n",
            "Excluded 137873 customers with only one product.\n",
            "preparing training and test data... 2025-04-01 05:10:24.988418\n",
            "preparing validations... 2025-04-01 05:11:17.916842\n",
            "original 232920, original_unique_customers: 84815, train_df: 163027, test_df: 69893, total: 232920\n",
            "train_df 163027, train_unique_customers: 59370, x_train: 59370, y_train: 59370, total: 163027\n",
            "test_df 69893, test_unique_customers: 25445, x_test: 25445, y_test: 25445, total: 69893\n",
            "Data Process Finished!... 2025-04-01 05:11:17.931552\n"
          ]
        }
      ],
      "source": [
        "def load_data():\n",
        "  print(f\"loading data... {datetime.now()}\")\n",
        "  df_orign = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/recommender_data.xlsx')\n",
        "  print(f\"data shape: {df_orign.shape}\")\n",
        "  print(f\"data loaded... {datetime.now()}\")\n",
        "  return df_orign\n",
        "\n",
        "def remove_outliers(df):\n",
        "  df_no_out = df.copy()\n",
        "  print(f\"removing outliers started... {datetime.now()}\")\n",
        "\n",
        "  # Compute 99th percentiles for numeric columns\n",
        "  caps = {\n",
        "      'income_group': df_no_out['income_group'].quantile(0.99),\n",
        "      'age_group': df_no_out['age_group'].quantile(0.99),\n",
        "      'cantidad_transacciones': df_no_out['cantidad_transacciones'].quantile(0.99),\n",
        "      'monto_transaccion': df_no_out['monto_transaccion'].quantile(0.99),\n",
        "  }\n",
        "\n",
        "  # Apply capping\n",
        "  for col, cap in caps.items():\n",
        "      df_no_out[col] = df_no_out[col].clip(upper=cap)\n",
        "\n",
        "  # Adjust fecha_apertura to ensure year is at least 2011\n",
        "  df_no_out['fecha_apertura'] = df_no_out['fecha_apertura'].apply(\n",
        "      lambda x: x if x.year >= 2011 else x.replace(year=2011)\n",
        "  )\n",
        "\n",
        "  print(f\"removing outliers finished... {datetime.now()}\")\n",
        "\n",
        "  return df_no_out\n",
        "\n",
        "def print_max_min(df, message:str = None):\n",
        "\n",
        "  print(f\"print_max_min started... {datetime.now()}\")\n",
        "  if message is not None:\n",
        "    print(f\"{message}... {datetime.now()}\")\n",
        "\n",
        "  # Print min and max values for each column\n",
        "  print(\"\\nColumn Min & Max values after correction:\")\n",
        "  for col in ['income_group', 'age_group', 'cantidad_transacciones', 'monto_transaccion']:\n",
        "      print(f\"{col}: min={df[col].min()}, max={df[col].max()}\")\n",
        "\n",
        "  # For fecha_apertura, print min & max years\n",
        "  min_date = df['fecha_apertura'].min()\n",
        "  max_date = df['fecha_apertura'].max()\n",
        "  print(f\"fecha_apertura: min_year={min_date.year}, max_year={max_date.year}\")\n",
        "\n",
        "  print(f\"print_max_min finished... {datetime.now()}\")\n",
        "\n",
        "\n",
        "def prepare_data(df_orign):\n",
        "  print(f\"preparing data... {datetime.now()}\")\n",
        "  df = df_orign.copy()\n",
        "\n",
        "  # Load DataFrame (assuming df is already loaded)\n",
        "  df['producto'] = df['producto'].str.strip()\n",
        "\n",
        "  # Hardcoded mappings for categorical columns\n",
        "  producto_mapping = {\n",
        "      \"Accidentes Personales\": 0,\n",
        "      \"Ahorro Infantil\": 1,\n",
        "      \"Ahorro Internacional\": 2,\n",
        "      \"Ahorro Libreta\": 3,\n",
        "      \"Ahorro TDD\": 4,\n",
        "      \"BLACK\": 5,\n",
        "      \"Banesco Asistencia\": 6,\n",
        "      \"CLASICA\": 7,\n",
        "      \"Corriente Transaccional\": 8,\n",
        "      \"Depositos\": 9,\n",
        "      \"Depositos Internacional\": 10,\n",
        "      \"Desempleo/Incapacidad\": 11,\n",
        "      \"GOLD\": 12,\n",
        "      \"Hipotecario\": 13,\n",
        "      \"Hospitalizacion\": 14,\n",
        "      \"INFINITE\": 15,\n",
        "      \"Nomina Electronica\": 16,\n",
        "      \"PLATINUM\": 17,\n",
        "      \"Personal\": 18,\n",
        "      \"Proteccion ATM\": 19,\n",
        "      \"SUPERCASHBACK\": 20,\n",
        "      \"Seguro Hogar\": 21,\n",
        "      \"Seguro Vida\": 22,\n",
        "      \"Seguro por Cancer\": 23,\n",
        "      \"Ultimos Gastos\": 24,\n",
        "      \"Vehiculo\": 25\n",
        "  }\n",
        "\n",
        "  education_mapping = {\n",
        "      \"Doctorate\": 0,\n",
        "      \"Master\": 1,\n",
        "      \"Secondary\": 2,\n",
        "      \"Undergraduate\": 3\n",
        "  }\n",
        "\n",
        "  aplicacion_mapping = {\n",
        "      \"CERTIFICADOS\": 0,\n",
        "      \"CUENTAS\": 1,\n",
        "      \"INSURANCE\": 2,\n",
        "      \"PRESTAMOS\": 3,\n",
        "      \"TDC\": 4\n",
        "  }\n",
        "\n",
        "  df['producto_encoded'] = df['producto'].map(producto_mapping)\n",
        "  df['education_level_encoded'] = df['education_level'].map(education_mapping)\n",
        "  df['aplicacion_encoded'] = df['aplicacion'].map(aplicacion_mapping)\n",
        "\n",
        "  # 2. Handle Missing Values in income_group and age_group\n",
        "  df['income_group'].replace(0, np.nan, inplace=True)\n",
        "  df['age_group'].replace(0, np.nan, inplace=True)\n",
        "\n",
        "  df['income_group_filled'] = df['income_group'].fillna(df['income_group'].median())\n",
        "  df['age_group_filled'] = df['age_group'].fillna(df['age_group'].median())\n",
        "\n",
        "  # 3. Fill Null Values in Transaction-related Columns\n",
        "  transaction_columns = ['fecha_transaccion', 'cantidad_transacciones', 'monto_transaccion']\n",
        "\n",
        "  date_evaluation = datetime(2025, 2, 28)\n",
        "\n",
        "  df[transaction_columns] = df[transaction_columns].replace(0, np.nan)\n",
        "  df[transaction_columns] = df[transaction_columns].replace(\"\", np.nan)\n",
        "\n",
        "  df['cantidad_transacciones_filled'] = df['cantidad_transacciones'].fillna(df['cantidad_transacciones'].median())\n",
        "  df['monto_transaccion_filled'] = df['monto_transaccion'].fillna(df['monto_transaccion'].median())\n",
        "\n",
        "  # Ensure 'fecha_transaccion' is in datetime format\n",
        "  df[\"fecha_transaccion\"] = pd.to_datetime(df[\"fecha_transaccion\"], errors='coerce')\n",
        "\n",
        "  # Calculate Time Since Last Activity, NaT will result in NaN in the new column\n",
        "  df[\"days_since_last_transaction\"] = (date_evaluation - df[\"fecha_transaccion\"]).dt.days\n",
        "  df['days_since_last_transaction_filled'] = df['days_since_last_transaction'].fillna(df['days_since_last_transaction'].median())\n",
        "\n",
        "  # Ensure 'fecha_transaccion' is in datetime format\n",
        "  df[\"fecha_apertura\"] = pd.to_datetime(df[\"fecha_apertura\"], errors='coerce')\n",
        "\n",
        "  # Calculate Time Since Last Activity, NaT will result in NaN in the new column\n",
        "  df[\"days_since_product_opened\"] = (date_evaluation - df[\"fecha_apertura\"]).dt.days\n",
        "\n",
        "  # Normalize numerical features\n",
        "  scaler = StandardScaler()\n",
        "  df[\"income_group_scaled\"] = scaler.fit_transform(df[[\"income_group_filled\"]])\n",
        "  df[\"age_group_scaled\"] = scaler.fit_transform(df[[\"age_group_filled\"]])\n",
        "  df[\"cantidad_transacciones_scaled\"] = scaler.fit_transform(df[[\"cantidad_transacciones_filled\"]])\n",
        "  df[\"monto_transaccion_scaled\"] = scaler.fit_transform(df[[\"monto_transaccion_filled\"]])\n",
        "  df[\"days_since_last_transaction_scaled\"] = scaler.fit_transform(df[[\"days_since_last_transaction_filled\"]])\n",
        "  df[\"days_since_product_opened_scaled\"] = scaler.fit_transform(df[[\"days_since_product_opened\"]])\n",
        "\n",
        "  # Sort by customer and fecha_apertura to ensure chronological order\n",
        "  df.sort_values(by=['customer_id', 'fecha_apertura'], inplace=True)\n",
        "\n",
        "  # Create the sequence column within each customer group\n",
        "  df['purchase_sequence'] = df.groupby('customer_id').cumcount() + 1\n",
        "\n",
        "  drop_columns = [\n",
        "      # \"customer_id\",\n",
        "      \"income_group\",\"age_group\",\"education_level\",\"aplicacion\",\"producto\",\"fecha_apertura\",\"fecha_transaccion\",\"cantidad_transacciones\",\"monto_transaccion\",\n",
        "      \"income_group_filled\",\"age_group_filled\",\"cantidad_transacciones_filled\",\"monto_transaccion_filled\",\"days_since_last_transaction\",\"days_since_last_transaction_filled\",\"days_since_product_opened\"\n",
        "  ]\n",
        "\n",
        "  # Count the number of products per customer\n",
        "  customer_product_counts = df['customer_id'].value_counts()\n",
        "\n",
        "  # Get the list of customers who have only one product\n",
        "  customers_to_exclude = customer_product_counts[customer_product_counts == 1].index\n",
        "\n",
        "  # Exclude these customers from the dataframe\n",
        "  df = df[~df['customer_id'].isin(customers_to_exclude)]\n",
        "\n",
        "  # Reset index after filtering (optional)\n",
        "  df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  # Print confirmation\n",
        "  print(f\"Excluded {len(customers_to_exclude)} customers with only one product.\")\n",
        "\n",
        "  df.drop(columns=drop_columns, inplace=True)\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "def prepare_train_test_dataset(df):\n",
        "  print(f\"preparing training and test data... {datetime.now()}\")\n",
        "  # Get unique customer IDs\n",
        "  unique_customers = df['customer_id'].unique()\n",
        "\n",
        "  # Split unique customers into training and testing sets\n",
        "  train_customers, test_customers = train_test_split(unique_customers, test_size=0.3, random_state=42)\n",
        "\n",
        "  # Create training and testing DataFrames\n",
        "  train_df = df[df['customer_id'].isin(train_customers)]\n",
        "  test_df = df[df['customer_id'].isin(test_customers)]\n",
        "\n",
        "  train_df.sort_values(by=['customer_id', 'purchase_sequence'], inplace=True)\n",
        "  train_grouped = train_df.groupby(\"customer_id\")\n",
        "  x_train = [group.drop(columns=['customer_id']).iloc[:-1].values.tolist() for _, group in train_grouped]\n",
        "  y_train = [group.iloc[-1]['producto_encoded'] for _, group in train_grouped]\n",
        "\n",
        "  test_df.sort_values(by=['customer_id', 'purchase_sequence'], inplace=True)\n",
        "  test_grouped = test_df.groupby(\"customer_id\")\n",
        "  x_test = [group.drop(columns=['customer_id']).iloc[:-1].values.tolist() for _, group in test_grouped]\n",
        "  y_test = [group.iloc[-1]['producto_encoded'] for _, group in test_grouped]\n",
        "  return train_df, test_df, x_train, y_train, x_test, y_test\n",
        "\n",
        "def print_data_Validations(df, train_df, test_df, x_train, y_train, x_test, y_test):\n",
        "  print(f\"preparing validations... {datetime.now()}\")\n",
        "  print(f\"original {len(df)}, original_unique_customers: {len(df['customer_id'].unique())}, train_df: {len(train_df)}, test_df: {len(test_df)}, total: {len(train_df)+len(test_df)}\")\n",
        "  print(f\"train_df {len(train_df)}, train_unique_customers: {len(train_df['customer_id'].unique())}, x_train: {len(x_train)}, y_train: {len(y_train)}, total: {sum(len(x) for x in x_train) + len(y_train)}\")\n",
        "  print(f\"test_df {len(test_df)}, test_unique_customers: {len(test_df['customer_id'].unique())}, x_test: {len(x_test)}, y_test: {len(y_test)}, total: {sum(len(x) for x in x_test) + len(y_test)}\")\n",
        "\n",
        "df_orig = load_data()\n",
        "print_max_min(df_orig, \"Original Dataset!\")\n",
        "df_orig_no_outliers = remove_outliers(df_orig)\n",
        "print_max_min(df_orig_no_outliers, \"Dataset with Outliers Removed!\")\n",
        "df = prepare_data(df_orig_no_outliers)\n",
        "# df = prepare_data(df_orig)\n",
        "train_df, test_df, x_train, y_train, x_test, y_test = prepare_train_test_dataset(df)\n",
        "print_data_Validations(df, train_df, test_df, x_train, y_train, x_test, y_test)\n",
        "print(f\"Data Process Finished!... {datetime.now()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGI0tBmR9DDm",
        "outputId": "8c7a48c1-f5dd-4f8b-d0bb-f68bdf23db5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 12ms/step - accuracy: 0.3888 - loss: 2.1553 - val_accuracy: 0.4787 - val_loss: 1.7446\n",
            "Epoch 2/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.4948 - loss: 1.6900 - val_accuracy: 0.4879 - val_loss: 1.6686\n",
            "Epoch 3/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.5111 - loss: 1.6072 - val_accuracy: 0.5087 - val_loss: 1.6099\n",
            "Epoch 4/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 10ms/step - accuracy: 0.5189 - loss: 1.5748 - val_accuracy: 0.5111 - val_loss: 1.6069\n",
            "Epoch 5/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.5291 - loss: 1.5391 - val_accuracy: 0.5189 - val_loss: 1.5591\n",
            "Epoch 6/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.5362 - loss: 1.5081 - val_accuracy: 0.5247 - val_loss: 1.5425\n",
            "Epoch 7/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.5387 - loss: 1.4916 - val_accuracy: 0.5234 - val_loss: 1.5363\n",
            "Epoch 8/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 10ms/step - accuracy: 0.5481 - loss: 1.4590 - val_accuracy: 0.5317 - val_loss: 1.5295\n",
            "Epoch 9/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.5479 - loss: 1.4517 - val_accuracy: 0.5328 - val_loss: 1.5060\n",
            "Epoch 10/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.5543 - loss: 1.4308 - val_accuracy: 0.5338 - val_loss: 1.5061\n",
            "Epoch 11/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.5584 - loss: 1.4177 - val_accuracy: 0.5368 - val_loss: 1.5026\n",
            "Epoch 12/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.5586 - loss: 1.4057 - val_accuracy: 0.5341 - val_loss: 1.5014\n",
            "Epoch 13/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.5614 - loss: 1.3988 - val_accuracy: 0.5370 - val_loss: 1.4768\n",
            "Epoch 14/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.5568 - loss: 1.4006 - val_accuracy: 0.5380 - val_loss: 1.4850\n",
            "Epoch 15/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.5645 - loss: 1.3838 - val_accuracy: 0.5387 - val_loss: 1.4800\n",
            "Epoch 16/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.5653 - loss: 1.3769 - val_accuracy: 0.5424 - val_loss: 1.4759\n",
            "Epoch 17/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.5668 - loss: 1.3687 - val_accuracy: 0.5419 - val_loss: 1.4782\n",
            "Epoch 18/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.5727 - loss: 1.3564 - val_accuracy: 0.5346 - val_loss: 1.4899\n",
            "Epoch 19/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.5735 - loss: 1.3451 - val_accuracy: 0.5461 - val_loss: 1.4620\n",
            "Epoch 20/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.5720 - loss: 1.3412 - val_accuracy: 0.5435 - val_loss: 1.4713\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79f7807f3350>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "#model #1\n",
        "\n",
        "# Pad sequences to make them equal length\n",
        "x_train_padded = pad_sequences(x_train, padding=\"post\", dtype=\"float32\")\n",
        "x_test_padded = pad_sequences(x_test, padding=\"post\", dtype=\"float32\")\n",
        "\n",
        "# Convert targets to numpy arrays\n",
        "y_train = np.array(y_train, dtype=np.int32)\n",
        "y_test = np.array(y_test, dtype=np.int32)\n",
        "\n",
        "# Build Neural Network Model (Replaced Flatten with LSTM or Pooling)\n",
        "model = keras.Sequential([\n",
        "    LSTM(64, return_sequences=True, input_shape=(x_train_padded.shape[1], x_train_padded.shape[2])),\n",
        "    LSTM(32),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(len(df[\"producto_encoded\"].unique()), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile Model\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train Model\n",
        "model.fit(x_train_padded, y_train, epochs=20, batch_size=32, validation_data=(x_test_padded, y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model #1 Evaluation\n",
        "\n",
        "# --------------------\n",
        "# 1. Function to Calculate Top-K Accuracy\n",
        "# --------------------\n",
        "def top_k_accuracy(y_true, y_pred, k=5):\n",
        "    \"\"\"\n",
        "    Calculates the percentage of times the actual product is in the top K predictions.\n",
        "\n",
        "    :param y_true: Array of true product indices\n",
        "    :param y_pred: Array of predicted probabilities for each product\n",
        "    :param k: Number of top predictions to consider\n",
        "    :return: Top-K Accuracy score\n",
        "    \"\"\"\n",
        "    correct_predictions = 0\n",
        "    total_samples = len(y_true)\n",
        "\n",
        "    for true_label, pred_probs in zip(y_true, y_pred):\n",
        "        top_k_predictions = np.argsort(pred_probs)[-k:][::-1]  # Get top K product indices\n",
        "        if true_label in top_k_predictions:\n",
        "            correct_predictions += 1\n",
        "\n",
        "    return correct_predictions / total_samples\n",
        "\n",
        "# --------------------\n",
        "# 2. Get Model Predictions\n",
        "# --------------------\n",
        "y_pred_probs = model.predict(x_test_padded)  # Predict probabilities for all test samples\n",
        "# y_pred_probs = model.predict([x_test_product_ids_padded, x_test_extra_features_padded])  # Predict probabilities for all test samples\n",
        "\n",
        "# --------------------\n",
        "# 3. Evaluate the Model Using Top-K Accuracy\n",
        "# --------------------\n",
        "top_1_acc = top_k_accuracy(y_test, y_pred_probs, k=1)  # Exact match accuracy\n",
        "top_3_acc = top_k_accuracy(y_test, y_pred_probs, k=3)  # Top 3 accuracy\n",
        "top_5_acc = top_k_accuracy(y_test, y_pred_probs, k=5)  # Top 5 accuracy\n",
        "top_8_acc = top_k_accuracy(y_test, y_pred_probs, k=8)  # Top 8 accuracy\n",
        "top_10_acc = top_k_accuracy(y_test, y_pred_probs, k=10)  # Top 10 accuracy\n",
        "\n",
        "print(f\"Top-1 Accuracy: {top_1_acc:.4f}\")  # Strict match accuracy\n",
        "print(f\"Top-3 Accuracy: {top_3_acc:.4f}\")  # If correct product is in top 3 predictions\n",
        "print(f\"Top-5 Accuracy: {top_5_acc:.4f}\")  # If correct product is in top 5 predictions\n",
        "print(f\"Top-8 Accuracy: {top_8_acc:.4f}\")  # If correct product is in top 8 predictions\n",
        "print(f\"Top-10 Accuracy: {top_10_acc:.4f}\")  # If correct product is in top 10 predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5t_96n3UNhK",
        "outputId": "2c9887c1-a0b8-4fc3-b101-de9be9a009a7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m796/796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step\n",
            "Top-1 Accuracy: 0.5435\n",
            "Top-3 Accuracy: 0.7698\n",
            "Top-5 Accuracy: 0.8634\n",
            "Top-8 Accuracy: 0.9370\n",
            "Top-10 Accuracy: 0.9644\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model #1 Evaluation - Including Recall and Precision!\n",
        "\n",
        "# --------------------\n",
        "# 1. Top-K Accuracy\n",
        "# --------------------\n",
        "def top_k_accuracy(y_true, y_pred, k=5):\n",
        "    correct_predictions = 0\n",
        "    total_samples = len(y_true)\n",
        "\n",
        "    for true_label, pred_probs in zip(y_true, y_pred):\n",
        "        top_k_predictions = np.argsort(pred_probs)[-k:][::-1]\n",
        "        if true_label in top_k_predictions:\n",
        "            correct_predictions += 1\n",
        "\n",
        "    return correct_predictions / total_samples\n",
        "\n",
        "\n",
        "# --------------------\n",
        "# 2. Precision@K\n",
        "# --------------------\n",
        "def precision_at_k(y_true, y_pred, k=5):\n",
        "    precision_scores = []\n",
        "\n",
        "    for true_label, pred_probs in zip(y_true, y_pred):\n",
        "        top_k_predictions = np.argsort(pred_probs)[-k:][::-1]\n",
        "        if true_label in top_k_predictions:\n",
        "            precision_scores.append(1 / k)  # 1 relevant item found in k predictions\n",
        "        else:\n",
        "            precision_scores.append(0)\n",
        "\n",
        "    return np.mean(precision_scores)\n",
        "\n",
        "\n",
        "# --------------------\n",
        "# 3. Recall@K\n",
        "# --------------------\n",
        "def recall_at_k(y_true, y_pred, k=5):\n",
        "    recall_scores = []\n",
        "\n",
        "    for true_label, pred_probs in zip(y_true, y_pred):\n",
        "        top_k_predictions = np.argsort(pred_probs)[-k:][::-1]\n",
        "        if true_label in top_k_predictions:\n",
        "            recall_scores.append(1)  # 1 out of 1 relevant item retrieved\n",
        "        else:\n",
        "            recall_scores.append(0)\n",
        "\n",
        "    return np.mean(recall_scores)\n",
        "\n",
        "# Predictions\n",
        "y_pred_probs = model.predict(x_test_padded)\n",
        "\n",
        "# K values\n",
        "k_values = [1, 3, 5, 8, 10]\n",
        "\n",
        "# Evaluate\n",
        "for k in k_values:\n",
        "    acc = top_k_accuracy(y_test, y_pred_probs, k)\n",
        "    prec = precision_at_k(y_test, y_pred_probs, k)\n",
        "    rec = recall_at_k(y_test, y_pred_probs, k)\n",
        "\n",
        "    print(f\"K={k} => Accuracy@{k}: {acc:.4f}, Precision@{k}: {prec:.4f}, Recall@{k}: {rec:.4f}\")\n"
      ],
      "metadata": {
        "id": "YNeQJOkja1p7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a1fda75-9a73-4be3-fe3a-dd449e4d793a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m796/796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step\n",
            "K=1 => Accuracy@1: 0.5435, Precision@1: 0.5435, Recall@1: 0.5435\n",
            "K=3 => Accuracy@3: 0.7698, Precision@3: 0.2566, Recall@3: 0.7698\n",
            "K=5 => Accuracy@5: 0.8634, Precision@5: 0.1727, Recall@5: 0.8634\n",
            "K=8 => Accuracy@8: 0.9370, Precision@8: 0.1171, Recall@8: 0.9370\n",
            "K=10 => Accuracy@10: 0.9644, Precision@10: 0.0964, Recall@10: 0.9644\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model #1 -  XGBoost - RandomForest\n",
        "\n",
        "dfresult = train_df.groupby('customer_id',as_index=False)['purchase_sequence'].max()\n",
        "x_df = train_df.merge(dfresult.set_index('customer_id'), on=['customer_id','purchase_sequence'],how='left',indicator=True).query(\"_merge == 'left_only'\").drop(columns=['_merge'])\n",
        "last_product = train_df[['customer_id','producto_encoded','purchase_sequence']].merge(dfresult.set_index('customer_id'), on=['customer_id','purchase_sequence'],how='inner')\n",
        "y_df = x_df[['customer_id']].merge(last_product.set_index('customer_id'), on=['customer_id'],how='left')[['customer_id','producto_encoded','purchase_sequence']]\n",
        "\n",
        "dfresult_test = test_df.groupby('customer_id',as_index=False)['purchase_sequence'].max()\n",
        "x_df_test = test_df.merge(dfresult_test.set_index('customer_id'), on=['customer_id','purchase_sequence'],how='left',indicator=True).query(\"_merge == 'left_only'\").drop(columns=['_merge'])\n",
        "last_product_test = test_df[['customer_id','producto_encoded','purchase_sequence']].merge(dfresult_test.set_index('customer_id'), on=['customer_id','purchase_sequence'],how='inner')\n",
        "y_df_test = x_df_test[['customer_id']].merge(last_product_test.set_index('customer_id'), on=['customer_id'],how='left')[['customer_id','producto_encoded','purchase_sequence']]\n",
        "\n",
        "# Aggregation functions for numeric features\n",
        "agg_functions = {\n",
        "    \"producto_encoded\": lambda x: list(x),  # Store product history as a list\n",
        "    'education_level_encoded': \"max\",  # Average education level\n",
        "    \"aplicacion_encoded\": \"nunique\",  # Number of unique apps used\n",
        "    \"income_group_scaled\": \"max\",  # Average income\n",
        "    \"age_group_scaled\": \"max\",  # Average age group\n",
        "    \"cantidad_transacciones_scaled\": [\"sum\", \"mean\"],  # Sum & avg transactions\n",
        "    \"monto_transaccion_scaled\": [\"sum\", \"mean\"],  # Sum & avg transaction amount\n",
        "    \"days_since_last_transaction_scaled\": \"min\",  # Most recent transaction\n",
        "    \"days_since_product_opened_scaled\": \"max\",  # Age of oldest product\n",
        "    \"purchase_sequence\": \"max\",  # Last purchase sequence number\n",
        "}\n",
        "\n",
        "apply_columns_name = [\n",
        "    \"customer_id\",\n",
        "    \"producto_encoded\",\n",
        "    'education_level_encoded',\n",
        "    \"aplicacion_encoded\",\n",
        "    \"income_group_scaled\",\n",
        "    \"age_group_scaled\",\n",
        "    \"cantidad_transacciones_scaled\",\n",
        "    \"cantidad_transacciones_scaled_mean\",\n",
        "    \"monto_transaccion_scaled\",\n",
        "    \"monto_transaccion_scaled_mean\",\n",
        "    \"days_since_last_transaction_scaled\",\n",
        "    \"days_since_product_opened_scaled\",\n",
        "    \"purchase_sequence\"\n",
        "]\n",
        "\n",
        "train_df_last_sequence = train_df.groupby('customer_id',as_index=False)['purchase_sequence'].max()\n",
        "x_train_df = train_df.merge(train_df_last_sequence.set_index('customer_id'), on=['customer_id','purchase_sequence'],how='left',indicator=True).query(\"_merge == 'left_only'\").drop(columns=['_merge'])\n",
        "\n",
        "test_df_last_sequence = test_df.groupby('customer_id',as_index=False)['purchase_sequence'].max()\n",
        "x_test_df = test_df.merge(test_df_last_sequence.set_index('customer_id'), on=['customer_id','purchase_sequence'],how='left',indicator=True).query(\"_merge == 'left_only'\").drop(columns=['_merge'])\n",
        "\n",
        "# Apply aggregation\n",
        "x_train_df_agg = x_train_df.groupby(\"customer_id\").agg(agg_functions,as_index=False).reset_index()\n",
        "x_test_df_agg  = x_test_df.groupby(\"customer_id\").agg(agg_functions,as_index=False).reset_index()\n",
        "\n",
        "x_train_df_agg.columns = apply_columns_name\n",
        "x_test_df_agg.columns = apply_columns_name\n",
        "\n",
        "y_train_df = train_df[['customer_id','producto_encoded','purchase_sequence']].merge(train_df_last_sequence.set_index('customer_id'), on=['customer_id','purchase_sequence'],how='inner')\n",
        "y_test_df  = test_df[['customer_id','producto_encoded','purchase_sequence']].merge(test_df_last_sequence.set_index('customer_id'), on=['customer_id','purchase_sequence'],how='inner')\n",
        "\n",
        "x_train_exploded_df = x_train_df_agg.explode('producto_encoded').reset_index(drop=True)\n",
        "x_train_one_hot_encoded_df = pd.get_dummies(x_train_exploded_df, columns=['producto_encoded'], prefix='product', drop_first=True)\n",
        "x_train_final_df = x_train_one_hot_encoded_df.groupby('customer_id').max().reset_index()\n",
        "\n",
        "x_test_exploded_df = x_test_df_agg.explode('producto_encoded').reset_index(drop=True)\n",
        "x_test_one_hot_encoded_df = pd.get_dummies(x_test_exploded_df, columns=['producto_encoded'], prefix='product', drop_first=True)\n",
        "x_test_final_df = x_test_one_hot_encoded_df.groupby('customer_id').max().reset_index()\n",
        "\n",
        "X = x_train_final_df.drop(columns=['customer_id'])\n",
        "y = y_train_df['producto_encoded']\n",
        "X_test = x_test_final_df.drop(columns=['customer_id'])\n",
        "y_test = y_test_df['producto_encoded']\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42, enable_categorical=True)\n",
        "xgb_model.fit(X,y)\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X, y)\n",
        "\n",
        "# 6. Obtener probabilidades de predicción para X_test\n",
        "y_proba = xgb_model.predict_proba(X_test)\n",
        "\n",
        "# 7. Obtener las 5 clases con mayor probabilidad para cada muestra\n",
        "top_5_indices = np.argsort(y_proba, axis=1)[:, -5:]  # Obtiene los índices de las 5 mayores probabilidades\n",
        "\n",
        "# 8. Verificar cuántas veces el verdadero valor está en las 5 mejores predicciones\n",
        "top_5_accuracy = np.mean([y_test.iloc[i] in top_5_indices[i] for i in range(len(y_test))])\n",
        "\n",
        "# 9. Imprimir la precisión de top-5\n",
        "print(f\"XGBoost Top-5 Accuracy: {top_5_accuracy:.4f}\")\n",
        "\n",
        "# 10. Evaluar precisión\n",
        "xgb_accuracy = xgb_model.score(X_test, y_test)\n",
        "print(f\"XGBoost Accuracy: {xgb_accuracy:.4f}\")\n",
        "\n",
        "# 6. Obtener probabilidades de predicción para X_test\n",
        "y_proba = rf_model.predict_proba(X_test)\n",
        "\n",
        "# 7. Obtener las 5 clases con mayor probabilidad para cada muestra\n",
        "top_5_indices = np.argsort(y_proba, axis=1)[:, -5:]  # Obtiene los índices de las 5 mayores probabilidades\n",
        "\n",
        "# 8. Verificar cuántas veces el verdadero valor está en las 5 mejores predicciones\n",
        "top_5_accuracy = np.mean([y_test.iloc[i] in top_5_indices[i] for i in range(len(y_test))])\n",
        "\n",
        "# 9. Imprimir la precisión de top-5\n",
        "print(f\"Random Forest Top-5 Accuracy: {top_5_accuracy:.4f}\")\n",
        "\n",
        "# 10. Evaluar precisión\n",
        "rf_accuracy = rf_model.score(X_test, y_test)\n",
        "print(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SX4JY4p0fJ_M",
        "outputId": "38889959-2a3b-4547-87a9-abe0f71f7403"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Top-5 Accuracy: 0.8861\n",
            "XGBoost Accuracy: 0.5600\n",
            "Random Forest Top-5 Accuracy: 0.8567\n",
            "Random Forest Accuracy: 0.5439\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "history_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}