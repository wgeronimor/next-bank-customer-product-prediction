{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timedelta\n",
        "\n",
        "print(f\"Installing libraries... {datetime.now()}\")\n",
        "!pip install openpyxl --quiet\n",
        "!pip install tensorflow scikit-learn --quiet\n",
        "!pip install imbalanced-learn --quiet\n",
        "!pip install xgboost --quiet\n",
        "\n",
        "print(f\"Loading require libraries... {datetime.now()}\")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout, BatchNormalization, Input, Flatten, GlobalAveragePooling1D\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "ER7iyT8C1JfQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2739d087-b006-451b-cc45-a1cbed0139cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing libraries... 2025-03-23 18:32:35.625437\n",
            "Loading require libraries... 2025-03-23 18:32:41.049468\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzQyrllkhF19",
        "outputId": "e999ae42-471e-4440-b8ff-67215ffe4a0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading data... 2025-03-23 18:32:49.501176\n",
            "preparing data... 2025-03-23 18:33:25.573246\n",
            "Excluded 137873 customers with only one product.\n",
            "preparing training and test data... 2025-03-23 18:33:26.165644\n",
            "preparing validations... 2025-03-23 18:34:16.662818\n",
            "original 232920, original_unique_customers: 84815, train_df: 163027, test_df: 69893, total: 232920\n",
            "train_df 163027, train_unique_customers: 59370, x_train: 59370, y_train: 59370, total: 163027\n",
            "test_df 69893, test_unique_customers: 25445, x_test: 25445, y_test: 25445, total: 69893\n"
          ]
        }
      ],
      "source": [
        "def load_data():\n",
        "  print(f\"loading data... {datetime.now()}\")\n",
        "  df_orign = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/recommender_data.xlsx')\n",
        "  return df_orign\n",
        "\n",
        "def prepare_data(df_orign):\n",
        "  print(f\"preparing data... {datetime.now()}\")\n",
        "  df = df_orign.copy()\n",
        "\n",
        "  # Load DataFrame (assuming df is already loaded)\n",
        "  df['producto'] = df['producto'].str.strip()\n",
        "\n",
        "  # Hardcoded mappings for categorical columns\n",
        "  producto_mapping = {\n",
        "      \"Accidentes Personales\": 0,\n",
        "      \"Ahorro Infantil\": 1,\n",
        "      \"Ahorro Internacional\": 2,\n",
        "      \"Ahorro Libreta\": 3,\n",
        "      \"Ahorro TDD\": 4,\n",
        "      \"BLACK\": 5,\n",
        "      \"Banesco Asistencia\": 6,\n",
        "      \"CLASICA\": 7,\n",
        "      \"Corriente Transaccional\": 8,\n",
        "      \"Depositos\": 9,\n",
        "      \"Depositos Internacional\": 10,\n",
        "      \"Desempleo/Incapacidad\": 11,\n",
        "      \"GOLD\": 12,\n",
        "      \"Hipotecario\": 13,\n",
        "      \"Hospitalizacion\": 14,\n",
        "      \"INFINITE\": 15,\n",
        "      \"Nomina Electronica\": 16,\n",
        "      \"PLATINUM\": 17,\n",
        "      \"Personal\": 18,\n",
        "      \"Proteccion ATM\": 19,\n",
        "      \"SUPERCASHBACK\": 20,\n",
        "      \"Seguro Hogar\": 21,\n",
        "      \"Seguro Vida\": 22,\n",
        "      \"Seguro por Cancer\": 23,\n",
        "      \"Ultimos Gastos\": 24,\n",
        "      \"Vehiculo\": 25\n",
        "  }\n",
        "\n",
        "  education_mapping = {\n",
        "      \"Doctorate\": 0,\n",
        "      \"Master\": 1,\n",
        "      \"Secondary\": 2,\n",
        "      \"Undergraduate\": 3\n",
        "  }\n",
        "\n",
        "  aplicacion_mapping = {\n",
        "      \"CERTIFICADOS\": 0,\n",
        "      \"CUENTAS\": 1,\n",
        "      \"INSURANCE\": 2,\n",
        "      \"PRESTAMOS\": 3,\n",
        "      \"TDC\": 4\n",
        "  }\n",
        "\n",
        "  df['producto_encoded'] = df['producto'].map(producto_mapping)\n",
        "  df['education_level_encoded'] = df['education_level'].map(education_mapping)\n",
        "  df['aplicacion_encoded'] = df['aplicacion'].map(aplicacion_mapping)\n",
        "\n",
        "  # 2. Handle Missing Values in income_group and age_group\n",
        "  df['income_group'].replace(0, np.nan, inplace=True)\n",
        "  df['age_group'].replace(0, np.nan, inplace=True)\n",
        "\n",
        "  df['income_group_filled'] = df['income_group'].fillna(df['income_group'].median())\n",
        "  df['age_group_filled'] = df['age_group'].fillna(df['age_group'].median())\n",
        "\n",
        "  # 3. Fill Null Values in Transaction-related Columns\n",
        "  transaction_columns = ['fecha_transaccion', 'cantidad_transacciones', 'monto_transaccion']\n",
        "\n",
        "  date_evaluation = datetime(2025, 2, 28)\n",
        "\n",
        "  df[transaction_columns] = df[transaction_columns].replace(0, np.nan)\n",
        "  df[transaction_columns] = df[transaction_columns].replace(\"\", np.nan)\n",
        "\n",
        "  df['cantidad_transacciones_filled'] = df['cantidad_transacciones'].fillna(df['cantidad_transacciones'].median())\n",
        "  df['monto_transaccion_filled'] = df['monto_transaccion'].fillna(df['monto_transaccion'].median())\n",
        "\n",
        "  # Ensure 'fecha_transaccion' is in datetime format\n",
        "  df[\"fecha_transaccion\"] = pd.to_datetime(df[\"fecha_transaccion\"], errors='coerce')\n",
        "\n",
        "  # Calculate Time Since Last Activity, NaT will result in NaN in the new column\n",
        "  df[\"days_since_last_transaction\"] = (date_evaluation - df[\"fecha_transaccion\"]).dt.days\n",
        "  df['days_since_last_transaction_filled'] = df['days_since_last_transaction'].fillna(df['days_since_last_transaction'].median())\n",
        "\n",
        "  # Ensure 'fecha_transaccion' is in datetime format\n",
        "  df[\"fecha_apertura\"] = pd.to_datetime(df[\"fecha_apertura\"], errors='coerce')\n",
        "\n",
        "  # Calculate Time Since Last Activity, NaT will result in NaN in the new column\n",
        "  df[\"days_since_product_opened\"] = (date_evaluation - df[\"fecha_apertura\"]).dt.days\n",
        "\n",
        "  # Normalize numerical features\n",
        "  scaler = StandardScaler()\n",
        "  df[\"income_group_scaled\"] = scaler.fit_transform(df[[\"income_group_filled\"]])\n",
        "  df[\"age_group_scaled\"] = scaler.fit_transform(df[[\"age_group_filled\"]])\n",
        "  df[\"cantidad_transacciones_scaled\"] = scaler.fit_transform(df[[\"cantidad_transacciones_filled\"]])\n",
        "  df[\"monto_transaccion_scaled\"] = scaler.fit_transform(df[[\"monto_transaccion_filled\"]])\n",
        "  df[\"days_since_last_transaction_scaled\"] = scaler.fit_transform(df[[\"days_since_last_transaction_filled\"]])\n",
        "  df[\"days_since_product_opened_scaled\"] = scaler.fit_transform(df[[\"days_since_product_opened\"]])\n",
        "\n",
        "  # Sort by customer and fecha_apertura to ensure chronological order\n",
        "  df.sort_values(by=['customer_id', 'fecha_apertura'], inplace=True)\n",
        "\n",
        "  # Create the sequence column within each customer group\n",
        "  df['purchase_sequence'] = df.groupby('customer_id').cumcount() + 1\n",
        "\n",
        "  drop_columns = [\n",
        "      # \"customer_id\",\n",
        "      \"income_group\",\"age_group\",\"education_level\",\"aplicacion\",\"producto\",\"fecha_apertura\",\"fecha_transaccion\",\"cantidad_transacciones\",\"monto_transaccion\",\n",
        "      \"income_group_filled\",\"age_group_filled\",\"cantidad_transacciones_filled\",\"monto_transaccion_filled\",\"days_since_last_transaction\",\"days_since_last_transaction_filled\",\"days_since_product_opened\"\n",
        "  ]\n",
        "\n",
        "  # Count the number of products per customer\n",
        "  customer_product_counts = df['customer_id'].value_counts()\n",
        "\n",
        "  # Get the list of customers who have only one product\n",
        "  customers_to_exclude = customer_product_counts[customer_product_counts == 1].index\n",
        "\n",
        "  # Exclude these customers from the dataframe\n",
        "  df = df[~df['customer_id'].isin(customers_to_exclude)]\n",
        "\n",
        "  # Reset index after filtering (optional)\n",
        "  df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  # Print confirmation\n",
        "  print(f\"Excluded {len(customers_to_exclude)} customers with only one product.\")\n",
        "\n",
        "  df.drop(columns=drop_columns, inplace=True)\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "def prepare_train_test_dataset(df):\n",
        "  print(f\"preparing training and test data... {datetime.now()}\")\n",
        "  # Get unique customer IDs\n",
        "  unique_customers = df['customer_id'].unique()\n",
        "\n",
        "  # Split unique customers into training and testing sets\n",
        "  train_customers, test_customers = train_test_split(unique_customers, test_size=0.3, random_state=42)\n",
        "\n",
        "  # Create training and testing DataFrames\n",
        "  train_df = df[df['customer_id'].isin(train_customers)]\n",
        "  test_df = df[df['customer_id'].isin(test_customers)]\n",
        "\n",
        "  train_df.sort_values(by=['customer_id', 'purchase_sequence'], inplace=True)\n",
        "  train_grouped = train_df.groupby(\"customer_id\")\n",
        "  x_train = [group.drop(columns=['customer_id']).iloc[:-1].values.tolist() for _, group in train_grouped]\n",
        "  y_train = [group.iloc[-1]['producto_encoded'] for _, group in train_grouped]\n",
        "\n",
        "  test_df.sort_values(by=['customer_id', 'purchase_sequence'], inplace=True)\n",
        "  test_grouped = test_df.groupby(\"customer_id\")\n",
        "  x_test = [group.drop(columns=['customer_id']).iloc[:-1].values.tolist() for _, group in test_grouped]\n",
        "  y_test = [group.iloc[-1]['producto_encoded'] for _, group in test_grouped]\n",
        "  return train_df, test_df, x_train, y_train, x_test, y_test\n",
        "\n",
        "def print_data_Validations(df, train_df, test_df, x_train, y_train, x_test, y_test):\n",
        "  print(f\"preparing validations... {datetime.now()}\")\n",
        "  print(f\"original {len(df)}, original_unique_customers: {len(df['customer_id'].unique())}, train_df: {len(train_df)}, test_df: {len(test_df)}, total: {len(train_df)+len(test_df)}\")\n",
        "  print(f\"train_df {len(train_df)}, train_unique_customers: {len(train_df['customer_id'].unique())}, x_train: {len(x_train)}, y_train: {len(y_train)}, total: {sum(len(x) for x in x_train) + len(y_train)}\")\n",
        "  print(f\"test_df {len(test_df)}, test_unique_customers: {len(test_df['customer_id'].unique())}, x_test: {len(x_test)}, y_test: {len(y_test)}, total: {sum(len(x) for x in x_test) + len(y_test)}\")\n",
        "\n",
        "df_orig = load_data()\n",
        "df = prepare_data(df_orig)\n",
        "train_df, test_df, x_train, y_train, x_test, y_test = prepare_train_test_dataset(df)\n",
        "print_data_Validations(df, train_df, test_df, x_train, y_train, x_test, y_test)\n",
        "print(f\"Data Process Finished!... {datetime.now()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGI0tBmR9DDm",
        "outputId": "dbf734b7-c1a4-49f4-bef1-4bf19a1180cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 10ms/step - accuracy: 0.3497 - loss: 2.2967 - val_accuracy: 0.4457 - val_loss: 1.8413\n",
            "Epoch 2/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 10ms/step - accuracy: 0.4796 - loss: 1.7507 - val_accuracy: 0.4940 - val_loss: 1.6819\n",
            "Epoch 3/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.5073 - loss: 1.6492 - val_accuracy: 0.5012 - val_loss: 1.6381\n",
            "Epoch 4/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.5152 - loss: 1.5982 - val_accuracy: 0.5138 - val_loss: 1.6027\n",
            "Epoch 5/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 10ms/step - accuracy: 0.5236 - loss: 1.5695 - val_accuracy: 0.5154 - val_loss: 1.5875\n",
            "Epoch 6/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.5282 - loss: 1.5476 - val_accuracy: 0.5176 - val_loss: 1.5750\n",
            "Epoch 7/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 10ms/step - accuracy: 0.5407 - loss: 1.5025 - val_accuracy: 0.5265 - val_loss: 1.5534\n",
            "Epoch 8/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.5422 - loss: 1.4947 - val_accuracy: 0.5311 - val_loss: 1.5346\n",
            "Epoch 9/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.5440 - loss: 1.4851 - val_accuracy: 0.5287 - val_loss: 1.5357\n",
            "Epoch 10/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.5492 - loss: 1.4639 - val_accuracy: 0.5298 - val_loss: 1.5338\n",
            "Epoch 11/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 9ms/step - accuracy: 0.5538 - loss: 1.4482 - val_accuracy: 0.5329 - val_loss: 1.5285\n",
            "Epoch 12/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 10ms/step - accuracy: 0.5567 - loss: 1.4334 - val_accuracy: 0.5373 - val_loss: 1.5032\n",
            "Epoch 13/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.5548 - loss: 1.4406 - val_accuracy: 0.5369 - val_loss: 1.5081\n",
            "Epoch 14/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.5517 - loss: 1.4356 - val_accuracy: 0.5346 - val_loss: 1.5051\n",
            "Epoch 15/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.5610 - loss: 1.4138 - val_accuracy: 0.5393 - val_loss: 1.5076\n",
            "Epoch 16/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.5613 - loss: 1.4116 - val_accuracy: 0.5414 - val_loss: 1.4836\n",
            "Epoch 17/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.5643 - loss: 1.4052 - val_accuracy: 0.5379 - val_loss: 1.4915\n",
            "Epoch 18/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 10ms/step - accuracy: 0.5638 - loss: 1.3986 - val_accuracy: 0.5383 - val_loss: 1.5072\n",
            "Epoch 19/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 10ms/step - accuracy: 0.5682 - loss: 1.3829 - val_accuracy: 0.5411 - val_loss: 1.4924\n",
            "Epoch 20/20\n",
            "\u001b[1m1856/1856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.5680 - loss: 1.3757 - val_accuracy: 0.5434 - val_loss: 1.4859\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a46fc1aadd0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "#model #1\n",
        "\n",
        "# Pad sequences to make them equal length\n",
        "x_train_padded = pad_sequences(x_train, padding=\"post\", dtype=\"float32\")\n",
        "x_test_padded = pad_sequences(x_test, padding=\"post\", dtype=\"float32\")\n",
        "\n",
        "# Convert targets to numpy arrays\n",
        "y_train = np.array(y_train, dtype=np.int32)\n",
        "y_test = np.array(y_test, dtype=np.int32)\n",
        "\n",
        "# Build Neural Network Model (Replaced Flatten with LSTM or Pooling)\n",
        "model = keras.Sequential([\n",
        "    LSTM(64, return_sequences=True, input_shape=(x_train_padded.shape[1], x_train_padded.shape[2])),\n",
        "    LSTM(32),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(len(df[\"producto_encoded\"].unique()), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile Model\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train Model\n",
        "model.fit(x_train_padded, y_train, epochs=20, batch_size=32, validation_data=(x_test_padded, y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model #1 Evaluation\n",
        "\n",
        "# --------------------\n",
        "# 1. Function to Calculate Top-K Accuracy\n",
        "# --------------------\n",
        "def top_k_accuracy(y_true, y_pred, k=5):\n",
        "    \"\"\"\n",
        "    Calculates the percentage of times the actual product is in the top K predictions.\n",
        "\n",
        "    :param y_true: Array of true product indices\n",
        "    :param y_pred: Array of predicted probabilities for each product\n",
        "    :param k: Number of top predictions to consider\n",
        "    :return: Top-K Accuracy score\n",
        "    \"\"\"\n",
        "    correct_predictions = 0\n",
        "    total_samples = len(y_true)\n",
        "\n",
        "    for true_label, pred_probs in zip(y_true, y_pred):\n",
        "        top_k_predictions = np.argsort(pred_probs)[-k:][::-1]  # Get top K product indices\n",
        "        if true_label in top_k_predictions:\n",
        "            correct_predictions += 1\n",
        "\n",
        "    return correct_predictions / total_samples\n",
        "\n",
        "# --------------------\n",
        "# 2. Get Model Predictions\n",
        "# --------------------\n",
        "y_pred_probs = model.predict(x_test_padded)  # Predict probabilities for all test samples\n",
        "# y_pred_probs = model.predict([x_test_product_ids_padded, x_test_extra_features_padded])  # Predict probabilities for all test samples\n",
        "\n",
        "# --------------------\n",
        "# 3. Evaluate the Model Using Top-K Accuracy\n",
        "# --------------------\n",
        "top_1_acc = top_k_accuracy(y_test, y_pred_probs, k=1)  # Exact match accuracy\n",
        "top_3_acc = top_k_accuracy(y_test, y_pred_probs, k=3)  # Top 3 accuracy\n",
        "top_5_acc = top_k_accuracy(y_test, y_pred_probs, k=5)  # Top 5 accuracy\n",
        "top_8_acc = top_k_accuracy(y_test, y_pred_probs, k=8)  # Top 8 accuracy\n",
        "top_10_acc = top_k_accuracy(y_test, y_pred_probs, k=10)  # Top 10 accuracy\n",
        "\n",
        "print(f\"Top-1 Accuracy: {top_1_acc:.4f}\")  # Strict match accuracy\n",
        "print(f\"Top-3 Accuracy: {top_3_acc:.4f}\")  # If correct product is in top 3 predictions\n",
        "print(f\"Top-5 Accuracy: {top_5_acc:.4f}\")  # If correct product is in top 5 predictions\n",
        "print(f\"Top-8 Accuracy: {top_8_acc:.4f}\")  # If correct product is in top 8 predictions\n",
        "print(f\"Top-10 Accuracy: {top_10_acc:.4f}\")  # If correct product is in top 10 predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5t_96n3UNhK",
        "outputId": "56fef9bf-0b51-4882-c875-e71e9845fa50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m796/796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step\n",
            "Top-1 Accuracy: 0.5434\n",
            "Top-3 Accuracy: 0.7661\n",
            "Top-5 Accuracy: 0.8599\n",
            "Top-8 Accuracy: 0.9335\n",
            "Top-10 Accuracy: 0.9611\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model #1 Evaluation - Including Recall and Precision!\n",
        "\n",
        "# --------------------\n",
        "# 1. Top-K Accuracy\n",
        "# --------------------\n",
        "def top_k_accuracy(y_true, y_pred, k=5):\n",
        "    correct_predictions = 0\n",
        "    total_samples = len(y_true)\n",
        "\n",
        "    for true_label, pred_probs in zip(y_true, y_pred):\n",
        "        top_k_predictions = np.argsort(pred_probs)[-k:][::-1]\n",
        "        if true_label in top_k_predictions:\n",
        "            correct_predictions += 1\n",
        "\n",
        "    return correct_predictions / total_samples\n",
        "\n",
        "\n",
        "# --------------------\n",
        "# 2. Precision@K\n",
        "# --------------------\n",
        "def precision_at_k(y_true, y_pred, k=5):\n",
        "    precision_scores = []\n",
        "\n",
        "    for true_label, pred_probs in zip(y_true, y_pred):\n",
        "        top_k_predictions = np.argsort(pred_probs)[-k:][::-1]\n",
        "        if true_label in top_k_predictions:\n",
        "            precision_scores.append(1 / k)  # 1 relevant item found in k predictions\n",
        "        else:\n",
        "            precision_scores.append(0)\n",
        "\n",
        "    return np.mean(precision_scores)\n",
        "\n",
        "\n",
        "# --------------------\n",
        "# 3. Recall@K\n",
        "# --------------------\n",
        "def recall_at_k(y_true, y_pred, k=5):\n",
        "    recall_scores = []\n",
        "\n",
        "    for true_label, pred_probs in zip(y_true, y_pred):\n",
        "        top_k_predictions = np.argsort(pred_probs)[-k:][::-1]\n",
        "        if true_label in top_k_predictions:\n",
        "            recall_scores.append(1)  # 1 out of 1 relevant item retrieved\n",
        "        else:\n",
        "            recall_scores.append(0)\n",
        "\n",
        "    return np.mean(recall_scores)\n",
        "\n",
        "# Predictions\n",
        "y_pred_probs = model.predict(x_test_padded)\n",
        "\n",
        "# K values\n",
        "k_values = [1, 3, 5, 8, 10]\n",
        "\n",
        "# Evaluate\n",
        "for k in k_values:\n",
        "    acc = top_k_accuracy(y_test, y_pred_probs, k)\n",
        "    prec = precision_at_k(y_test, y_pred_probs, k)\n",
        "    rec = recall_at_k(y_test, y_pred_probs, k)\n",
        "\n",
        "    print(f\"K={k} => Accuracy@{k}: {acc:.4f}, Precision@{k}: {prec:.4f}, Recall@{k}: {rec:.4f}\")\n"
      ],
      "metadata": {
        "id": "YNeQJOkja1p7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model #1 -  XGBoost - RandomForest\n",
        "\n",
        "dfresult = train_df.groupby('customer_id',as_index=False)['purchase_sequence'].max()\n",
        "x_df = train_df.merge(dfresult.set_index('customer_id'), on=['customer_id','purchase_sequence'],how='left',indicator=True).query(\"_merge == 'left_only'\").drop(columns=['_merge'])\n",
        "last_product = train_df[['customer_id','producto_encoded','purchase_sequence']].merge(dfresult.set_index('customer_id'), on=['customer_id','purchase_sequence'],how='inner')\n",
        "y_df = x_df[['customer_id']].merge(last_product.set_index('customer_id'), on=['customer_id'],how='left')[['customer_id','producto_encoded','purchase_sequence']]\n",
        "\n",
        "dfresult_test = test_df.groupby('customer_id',as_index=False)['purchase_sequence'].max()\n",
        "x_df_test = test_df.merge(dfresult_test.set_index('customer_id'), on=['customer_id','purchase_sequence'],how='left',indicator=True).query(\"_merge == 'left_only'\").drop(columns=['_merge'])\n",
        "last_product_test = test_df[['customer_id','producto_encoded','purchase_sequence']].merge(dfresult_test.set_index('customer_id'), on=['customer_id','purchase_sequence'],how='inner')\n",
        "y_df_test = x_df_test[['customer_id']].merge(last_product_test.set_index('customer_id'), on=['customer_id'],how='left')[['customer_id','producto_encoded','purchase_sequence']]\n",
        "\n",
        "# Aggregation functions for numeric features\n",
        "agg_functions = {\n",
        "    \"producto_encoded\": lambda x: list(x),  # Store product history as a list\n",
        "    'education_level_encoded': \"max\",  # Average education level\n",
        "    \"aplicacion_encoded\": \"nunique\",  # Number of unique apps used\n",
        "    \"income_group_scaled\": \"max\",  # Average income\n",
        "    \"age_group_scaled\": \"max\",  # Average age group\n",
        "    \"cantidad_transacciones_scaled\": [\"sum\", \"mean\"],  # Sum & avg transactions\n",
        "    \"monto_transaccion_scaled\": [\"sum\", \"mean\"],  # Sum & avg transaction amount\n",
        "    \"days_since_last_transaction_scaled\": \"min\",  # Most recent transaction\n",
        "    \"days_since_product_opened_scaled\": \"max\",  # Age of oldest product\n",
        "    \"purchase_sequence\": \"max\",  # Last purchase sequence number\n",
        "}\n",
        "\n",
        "apply_columns_name = [\n",
        "    \"customer_id\",\n",
        "    \"producto_encoded\",\n",
        "    'education_level_encoded',\n",
        "    \"aplicacion_encoded\",\n",
        "    \"income_group_scaled\",\n",
        "    \"age_group_scaled\",\n",
        "    \"cantidad_transacciones_scaled\",\n",
        "    \"cantidad_transacciones_scaled_mean\",\n",
        "    \"monto_transaccion_scaled\",\n",
        "    \"monto_transaccion_scaled_mean\",\n",
        "    \"days_since_last_transaction_scaled\",\n",
        "    \"days_since_product_opened_scaled\",\n",
        "    \"purchase_sequence\"\n",
        "]\n",
        "\n",
        "train_df_last_sequence = train_df.groupby('customer_id',as_index=False)['purchase_sequence'].max()\n",
        "x_train_df = train_df.merge(train_df_last_sequence.set_index('customer_id'), on=['customer_id','purchase_sequence'],how='left',indicator=True).query(\"_merge == 'left_only'\").drop(columns=['_merge'])\n",
        "\n",
        "test_df_last_sequence = test_df.groupby('customer_id',as_index=False)['purchase_sequence'].max()\n",
        "x_test_df = test_df.merge(test_df_last_sequence.set_index('customer_id'), on=['customer_id','purchase_sequence'],how='left',indicator=True).query(\"_merge == 'left_only'\").drop(columns=['_merge'])\n",
        "\n",
        "# Apply aggregation\n",
        "x_train_df_agg = x_train_df.groupby(\"customer_id\").agg(agg_functions,as_index=False).reset_index()\n",
        "x_test_df_agg  = x_test_df.groupby(\"customer_id\").agg(agg_functions,as_index=False).reset_index()\n",
        "\n",
        "x_train_df_agg.columns = apply_columns_name\n",
        "x_test_df_agg.columns = apply_columns_name\n",
        "\n",
        "y_train_df = train_df[['customer_id','producto_encoded','purchase_sequence']].merge(train_df_last_sequence.set_index('customer_id'), on=['customer_id','purchase_sequence'],how='inner')\n",
        "y_test_df  = test_df[['customer_id','producto_encoded','purchase_sequence']].merge(test_df_last_sequence.set_index('customer_id'), on=['customer_id','purchase_sequence'],how='inner')\n",
        "\n",
        "x_train_exploded_df = x_train_df_agg.explode('producto_encoded').reset_index(drop=True)\n",
        "x_train_one_hot_encoded_df = pd.get_dummies(x_train_exploded_df, columns=['producto_encoded'], prefix='product', drop_first=True)\n",
        "x_train_final_df = x_train_one_hot_encoded_df.groupby('customer_id').max().reset_index()\n",
        "\n",
        "x_test_exploded_df = x_test_df_agg.explode('producto_encoded').reset_index(drop=True)\n",
        "x_test_one_hot_encoded_df = pd.get_dummies(x_test_exploded_df, columns=['producto_encoded'], prefix='product', drop_first=True)\n",
        "x_test_final_df = x_test_one_hot_encoded_df.groupby('customer_id').max().reset_index()\n",
        "\n",
        "X = x_train_final_df.drop(columns=['customer_id'])\n",
        "y = y_train_df['producto_encoded']\n",
        "X_test = x_test_final_df.drop(columns=['customer_id'])\n",
        "y_test = y_test_df['producto_encoded']\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42, enable_categorical=True)\n",
        "xgb_model.fit(X,y)\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X, y)\n",
        "\n",
        "# 6. Obtener probabilidades de predicción para X_test\n",
        "y_proba = xgb_model.predict_proba(X_test)\n",
        "\n",
        "# 7. Obtener las 5 clases con mayor probabilidad para cada muestra\n",
        "top_5_indices = np.argsort(y_proba, axis=1)[:, -5:]  # Obtiene los índices de las 5 mayores probabilidades\n",
        "\n",
        "# 8. Verificar cuántas veces el verdadero valor está en las 5 mejores predicciones\n",
        "top_5_accuracy = np.mean([y_test.iloc[i] in top_5_indices[i] for i in range(len(y_test))])\n",
        "\n",
        "# 9. Imprimir la precisión de top-5\n",
        "print(f\"XGBoost Top-5 Accuracy: {top_5_accuracy:.4f}\")\n",
        "\n",
        "# 10. Evaluar precisión\n",
        "xgb_accuracy = xgb_model.score(X_test, y_test)\n",
        "print(f\"XGBoost Accuracy: {xgb_accuracy:.4f}\")\n",
        "\n",
        "# 6. Obtener probabilidades de predicción para X_test\n",
        "y_proba = rf_model.predict_proba(X_test)\n",
        "\n",
        "# 7. Obtener las 5 clases con mayor probabilidad para cada muestra\n",
        "top_5_indices = np.argsort(y_proba, axis=1)[:, -5:]  # Obtiene los índices de las 5 mayores probabilidades\n",
        "\n",
        "# 8. Verificar cuántas veces el verdadero valor está en las 5 mejores predicciones\n",
        "top_5_accuracy = np.mean([y_test.iloc[i] in top_5_indices[i] for i in range(len(y_test))])\n",
        "\n",
        "# 9. Imprimir la precisión de top-5\n",
        "print(f\"Random Forest Top-5 Accuracy: {top_5_accuracy:.4f}\")\n",
        "\n",
        "# 10. Evaluar precisión\n",
        "rf_accuracy = rf_model.score(X_test, y_test)\n",
        "print(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SX4JY4p0fJ_M",
        "outputId": "2312f388-5acd-4aa0-8896-7507b456b160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Top-5 Accuracy: 0.8852\n",
            "XGBoost Accuracy: 0.5604\n",
            "Random Forest Top-5 Accuracy: 0.8552\n",
            "Random Forest Accuracy: 0.5438\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "history_visible": true,
      "mount_file_id": "11vq80v4g1jhShCR4YvC2DPfs9ng47zXU",
      "authorship_tag": "ABX9TyMaHogxA5LTZg9CVfqFnc3n"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}